===== ./demo_hysteresis_inverse_PINN_staged_plot.m =====
%% demo_hysteresis_inverse_PINN_staged_plot.m
% 基于 BasePINN + HysteresisInversePINN 的分阶段训练 demo
% - 每阶段训练后画 train/test 拟合 + loss
% - 训练结束后画总的拟合 + loss + 滞回曲线

clear; clc; close all;
rng(0);

%% ===== 0. 一些训练超参数 =====
% 时间与采样
dt      = 1e-3;
T_end   = 1.2;
t       = (0:dt:T_end-dt).';
N       = numel(t);

train_ratio = 0.8;
N_train = floor(N * train_ratio);
t_train = t(1:N_train);
t_test  = t(N_train+1:end);

% 各阶段下采样 / 物理点抽样
data_stride_stage1 = 5;
data_stride_stage2 = 2;
phys_stride_stage2 = 5;
phys_stride_stage3 = 5;

% 分阶段训练轮数 / 学习率
epochs1 = 2000; lr1 = 1e-3;   % Data only
epochs2 = 3000; lr2 = 1e-3;   % Data + Phys
epochs3 = 1000; lr3 = 2e-4;   % Full loss

%% ===== 1. 生成全序列数据 (GPI 正向迟滞) =====
% 逆模型输入：u_all（位移），输出：v_all（电压）
v_all = 5 + 4*sin(2*pi*1*t) + 1.5*sin(2*pi*3*t);
v_all = max(0, min(10, v_all));   % 限幅 [0,10]

u_all = zeros(size(v_all));
Fr = [];
for k = 1:N
    [u_all(k), Fr] = GPI_Model(v_all(k), Fr);   % 这里调用你已有的 GPI_Model
end

u_train_full = u_all(1:N_train);
v_train_full = v_all(1:N_train);
u_test       = u_all(N_train+1:end);
v_test       = v_all(N_train+1:end);

X_test_dl = dlarray(single(u_test'));
Y_test_dl = dlarray(single(v_test'));

%% ===== 2. 正向迟滞参数（给 physicsFcn 用） =====
physParam.rd  = [-8 -6 -4 -2 0 2 4 6 8];
physParam.eta = [1 1 1 1 1.2437 -0.1103 -0.0787 -0.1742 -0.1767];
physParam.r   = [0 1 2 3 4 5];
physParam.P   = [0.7460 0.1860 0.0650 0.0855 0.0362 -0.1159];

%% ===== 3. 定义网络 =====
layers = [1 64 64 1];
dummyWeights = struct('lambdaData',1,'lambdaPhys',0,'lambdaMono',0,'lambdaSmooth',0);
pinn = HysteresisInversePINN(layers, physParam, dummyWeights, @physicsFcn_GPI_forward);

%% 工具：记录各阶段 lossHistory 范围
stageRanges = struct('name',{},'idx',{});

%% ===== 4. Stage 1: Data only + 强下采样 =====
disp("===== Stage 1: Data only =====");

idx_s1 = 1:data_stride_stage1:N_train;
u_s1 = u_train_full(idx_s1);
v_s1 = v_train_full(idx_s1);

X_s1_dl = dlarray(single(u_s1'));
Y_s1_dl = dlarray(single(v_s1'));

lossW1.lambdaData   = 1;
lossW1.lambdaPhys   = 0;
lossW1.lambdaMono   = 0;
lossW1.lambdaSmooth = 0;

prevLen = numel(pinn.lossHistory.total);
pinn.train(X_s1_dl, Y_s1_dl, X_s1_dl, ...
           @physicsFcn_GPI_forward, physParam, lossW1, ...
           epochs1, lr1);
newLen = numel(pinn.lossHistory.total);
stageRanges(end+1).name = 'Stage 1'; 
stageRanges(end).idx    = prevLen+1:newLen;

% --- Stage 1 绘图 ---
plot_stage_fit(pinn, 1, t_train, u_train_full, v_train_full, ...
               t_test, u_test, v_test);
plot_stage_loss(pinn, 1, stageRanges(end).idx);

%% ===== 5. Stage 2: Data + Physics，中等下采样 + 物理点抽样 =====
disp("===== Stage 2: Data + Physics =====");

idx_s2 = 1:data_stride_stage2:N_train;
u_s2 = u_train_full(idx_s2);
v_s2 = v_train_full(idx_s2);

X_s2_dl = dlarray(single(u_s2'));
Y_s2_dl = dlarray(single(v_s2'));

idx_phys2 = 1:phys_stride_stage2:numel(u_s2);
X_phys_s2_dl = X_s2_dl(:, idx_phys2);

lossW2.lambdaData   = 1;
lossW2.lambdaPhys   = 10;
lossW2.lambdaMono   = 0;
lossW2.lambdaSmooth = 0;

prevLen = numel(pinn.lossHistory.total);
pinn.train(X_s2_dl, Y_s2_dl, X_phys_s2_dl, ...
           @physicsFcn_GPI_forward, physParam, lossW2, ...
           epochs2, lr2);
newLen = numel(pinn.lossHistory.total);
stageRanges(end+1).name = 'Stage 2'; 
stageRanges(end).idx    = prevLen+1:newLen;

% --- Stage 2 绘图 ---
plot_stage_fit(pinn, 2, t_train, u_train_full, v_train_full, ...
               t_test, u_test, v_test);
plot_stage_loss(pinn, 2, stageRanges(end).idx);

%% ===== 6. Stage 3: Data + Physics + Mono + Smooth =====
disp("===== Stage 3: Full loss =====");

idx_s3 = idx_s2;          % 这里用和 Stage2 相同的下采样
u_s3   = u_s2;
v_s3   = v_s2;

X_s3_dl = dlarray(single(u_s3'));   % 输入 u
Y_s3_dl = dlarray(single(v_s3'));   % 输出 v  

idx_phys3     = 1:phys_stride_stage3:numel(u_s3);
X_phys_s3_dl  = X_s3_dl(:, idx_phys3);

lossW3.lambdaData   = 5;
lossW3.lambdaPhys   = 5;
lossW3.lambdaMono   = 0.1;
lossW3.lambdaSmooth = 0.001;


prevLen = numel(pinn.lossHistory.total);
pinn.train(X_s3_dl, Y_s3_dl, X_phys_s3_dl, ...
           @physicsFcn_GPI_forward, physParam, lossW3, ...
           epochs3, lr3);
newLen = numel(pinn.lossHistory.total);
stageRanges(end+1).name = 'Stage 3'; 
stageRanges(end).idx    = prevLen+1:newLen;

% --- Stage 3 绘图 ---
plot_stage_fit(pinn, 3, t_train, u_train_full, v_train_full, ...
               t_test, u_test, v_test);
plot_stage_loss(pinn, 3, stageRanges(end).idx);

%% ===== 7. 训练结束后：总拟合 + 总 loss + 滞回曲线 =====
disp("===== Final plots =====");

% ---- 总拟合：train + test ----
v_train_pred_dl = pinn.forward(dlarray(single(u_train_full')));
v_train_pred = double(extractdata(v_train_pred_dl)).';

v_test_pred_dl = pinn.forward(X_test_dl);
v_test_pred = double(extractdata(v_test_pred_dl)).';

mse_train = mean((v_train_pred - v_train_full).^2);
mse_test  = mean((v_test_pred  - v_test).^2);

figure('Name','Final Fit (Train + Test)');
subplot(2,1,1); hold on; box on;
plot(t_train, v_train_full, 'b', 'LineWidth',1.2);
plot(t_train, v_train_pred, 'r--','LineWidth',1.2);
xlabel('t [s]'); ylabel('Voltage v [V]');
title(sprintf('Train fit (MSE=%.2e)', mse_train));
legend('True v','PINN v_{pred}','Location','best');

subplot(2,1,2); hold on; box on;
plot(t_test, v_test, 'b', 'LineWidth',1.2);
plot(t_test, v_test_pred, 'r--','LineWidth',1.2);
xlabel('t [s]'); ylabel('Voltage v [V]');
title(sprintf('Test fit (MSE=%.2e)', mse_test));
legend('True v','PINN v_{pred}','Location','best');

% ---- 总 loss 曲线 ----
if ~isempty(pinn.lossHistory.components)
    it_all = pinn.lossHistory.iter;
    Lall   = pinn.lossHistory.components;   % 5 x K: [total,data,phys,mono,smooth]
    Ltotal = Lall(1,:);
    Ldata  = Lall(2,:);
    Lphys  = Lall(3,:);
    Lmono  = Lall(4,:);
    Lsmooth= Lall(5,:);

    figure('Name','Total Loss History');
    hold on; box on;
    plot(it_all, Ltotal, 'k','LineWidth',1.2);
    plot(it_all, Ldata,  'b--','LineWidth',1.0);
    plot(it_all, Lphys,  'r--','LineWidth',1.0);
    plot(it_all, Lmono,  'g--','LineWidth',1.0);
    plot(it_all, Lsmooth,'m--','LineWidth',1.0);
    xlabel('Iteration'); ylabel('Loss');
    legend('Total','Data','Phys','Mono','Smooth','Location','best');
    title('Overall Loss History');
    grid on;
end

% ---- 滞回曲线：真实 (v_all,u_all) vs PINN 逆 + 正向 GPI ----
% 先用逆模型预测 v_pred_all，再过 physicsFcn 得到 u_hat_all
v_pred_all_dl = pinn.forward(dlarray(single(u_all')));
v_pred_all = double(extractdata(v_pred_all_dl)).';

u_hat_all_dl = physicsFcn_GPI_forward(dlarray(single(v_pred_all')), physParam);
u_hat_all = double(extractdata(u_hat_all_dl)).';

figure('Name','Hysteresis Loop');
hold on; box on;
plot(v_all, u_all, 'b.', 'DisplayName','True hysteresis');
plot(v_pred_all, u_hat_all, 'r.', 'DisplayName','PINN inverse -> forward');
xlabel('Voltage v'); ylabel('Displacement u');
legend('Location','best');
title('Hysteresis loop comparison');
grid on;

%% ================== 辅助绘图函数 ==================
function plot_stage_fit(pinn, stageId, t_train, u_train, v_train, ...
                        t_test, u_test, v_test)

    % 预测 train / test
    v_train_pred = double(extractdata( ...
        pinn.forward(dlarray(single(u_train'))))).';
    v_test_pred = double(extractdata( ...
        pinn.forward(dlarray(single(u_test'))))).';

    mse_train = mean((v_train_pred - v_train).^2);
    mse_test  = mean((v_test_pred  - v_test).^2);

    figure('Name',sprintf('Stage %d Fit', stageId));
    subplot(2,1,1); hold on; box on;
    plot(t_train, v_train, 'b','LineWidth',1.2);
    plot(t_train, v_train_pred,'r--','LineWidth',1.2);
    xlabel('t [s]'); ylabel('Voltage v [V]');
    title(sprintf('Stage %d Train fit (MSE=%.2e)', stageId, mse_train));
    legend('True v','PINN v_{pred}','Location','best');

    subplot(2,1,2); hold on; box on;
    plot(t_test, v_test, 'b','LineWidth',1.2);
    plot(t_test, v_test_pred,'r--','LineWidth',1.2);
    xlabel('t [s]'); ylabel('Voltage v [V]');
    title(sprintf('Stage %d Test fit (MSE=%.2e)', stageId, mse_test));
    legend('True v','PINN v_{pred}','Location','best');
end

function plot_stage_loss(pinn, stageId, idxRange)
    if isempty(pinn.lossHistory.components)
        return;
    end
    it   = pinn.lossHistory.iter(idxRange);
    Lall = pinn.lossHistory.components(:, idxRange);
    Ltotal = Lall(1,:);
    Ldata  = Lall(2,:);
    Lphys  = Lall(3,:);
    Lmono  = Lall(4,:);
    Lsmooth= Lall(5,:);

    figure('Name',sprintf('Stage %d Loss', stageId));
    hold on; box on;
    plot(it, Ltotal, 'k','LineWidth',1.2);
    plot(it, Ldata,  'b--','LineWidth',1.0);
    plot(it, Lphys,  'r--','LineWidth',1.0);
    plot(it, Lmono,  'g--','LineWidth',1.0);
    plot(it, Lsmooth,'m--','LineWidth',1.0);
    xlabel('Iteration'); ylabel('Loss');
    legend('Total','Data','Phys','Mono','Smooth','Location','best');
    title(sprintf('Stage %d Loss History', stageId));
    grid on;
end
===== ./GPI_Model.m =====
function [y,Fr1_updated] = GPI_Model(v, Fr1_previous)
    %% v  % 0-10V,0.001s,1000hz
    
    rd = [-8    -6    -4    -2     0     2     4     6     8];
    eta = [1.0000    1.0000    1.0000    1.0000    1.2437   -0.1103   -0.0787   -0.1742   -0.1767];
    r=[0     1     2     3     4     5];
    n=length(r);
    P=[0.7460    0.1860    0.0650    0.0855    0.0362   -0.1159];

    Ad = zeros(length(rd),1);
    for z = 1:length(rd)
        if rd(z) > 0
            Ad(z) = max(v - rd(z), 0);
        elseif rd(z) < 0
            Ad(z) = min(v - rd(z), 0);
        else
            Ad(z) = v;
        end
    end
    w = eta * Ad;

    Fr1 = zeros(n, 1);
    % 对于每个阈值r(j)，更新Fr1(j)
    for j = 1:n
        if ~isempty(Fr1_previous)
            Fr1(j) = Fr1_previous(j);
        end
        F_inc = w;
        F_dec = w;
        A = F_inc - r(j);
        B = min(F_dec + r(j), Fr1(j));
        Fr1(j) = max(A, B); % Equation (1)
    end
    %% 输出y
    y = P*Fr1;
    Fr1_updated = Fr1;
end
===== ./physicsFcn_GPI_forward.m =====
%% ===== 正向迟滞算子（GPI）：dlarray 版本 =====
function u_hat = physicsFcn_GPI_forward(v_pred, physParam)
    rd  = physParam.rd;
    eta = physParam.eta;
    r   = physParam.r;
    P   = physParam.P;

    if ~isa(v_pred, 'dlarray')
        v_pred = dlarray(single(v_pred));
    else
        if ~isa(extractdata(v_pred), 'single')
            v_pred = dlarray(single(extractdata(v_pred)));
        end
    end

    N  = size(v_pred, 2);
    nR = numel(r);
    nD = numel(rd);

    Fr1_prev = zeros(nR, 1, 'single');
    u_hat_data = zeros(1, N, 'single');

    for k = 1:N
        v = v_pred(1,k);

        % Dead-zone
        Ad = zeros(nD,1,'single');
        for z = 1:nD
            if rd(z) > 0
                Ad(z) = max(v - rd(z), 0);
            elseif rd(z) < 0
                Ad(z) = min(v - rd(z), 0);
            else
                Ad(z) = v;
            end
        end
        w = eta * Ad;

        % Play operator
        Fr1 = zeros(nR,1,'single');
        for j = 1:nR
            A = w - r(j);
            B = min(w + r(j), Fr1_prev(j));
            Fr1(j) = max(A, B);
        end

        Fr1_prev = Fr1;
        u_hat_data(1,k) = P * Fr1;
    end

    u_hat = dlarray(u_hat_data);
end===== ./PINN/BasePINN.m =====
classdef BasePINN < handle
    %BASEPINN Generic PINN base class.
    %   - 管理参数初始化
    %   - 前向传播 (MLP)
    %   - Adam 训练 + 热启动
    %   - dlfeval 通过静态 wrapper 统一调用
    %
    % 子类必须实现:
    %   [loss, grads] = computeLoss(obj, params, X_data, Y_data, X_phys, physicsFcn, physParam, lossWeights)

    properties
        params          % W1,b1,...,WL,bL
        layers
        lossHistory     % 通用损失记录: struct('iter',[],'total',[],'components',[])
        iteration       % Adam 迭代计数（支持热启动）
        adamAvg
        adamAvgSq
    end

    methods
        %% 构造函数
        function obj = BasePINN(layers)
            obj.layers = layers;
            obj.params = obj.initParams(layers);
            [obj.adamAvg, obj.adamAvgSq] = obj.initAdamState(obj.params);

            % 通用的 loss 记录结构
            obj.lossHistory = struct( ...
                'iter',      [], ...   % 1 x K
                'total',     [], ...   % 1 x K
                'components', [] );    % n_terms x K

            obj.iteration = 0;
        end

        %% 前向预测，使用当前 obj.params
        function Y = forward(obj, X)
            Y = BasePINN.forwardWithParams(obj.params, X);
        end

        %% 训练（Adam），支持热启动
        function obj = train(obj, X_data, Y_data, X_phys, ...
                             physicsFcn, physParam, lossWeights, ...
                             epochs, lr)

            for ep = 1:epochs
                obj.iteration = obj.iteration + 1;

                % 通过静态 wrapper + dlfeval 调用 computeLoss
                [loss, grads] = dlfeval(@BasePINN.lossWrapper, ...
                    obj, obj.params, ...
                    X_data, Y_data, X_phys, ...
                    physicsFcn, physParam, lossWeights);

                % Adam 更新（逻辑和原 trainPINN 完全一致）
                fn = fieldnames(obj.params);
                for i = 1:numel(fn)
                    name = fn{i};

                    [obj.params.(name), ...
                     obj.adamAvg.(name), ...
                     obj.adamAvgSq.(name)] = ...
                        adamupdate( ...
                            obj.params.(name), grads.(name), ...
                            obj.adamAvg.(name), obj.adamAvgSq.(name), ...
                            obj.iteration, lr);
                end

                % ===== 通用记录损失（如果子类实现了 evalLossComponents）=====
                if ismethod(obj, 'evalLossComponents')
                    % 返回一个向量: [Ltotal, L1, L2, ...]
                    Lvec = dlfeval(@BasePINN.evalWrapper, ...
                                   obj, obj.params, ...
                                   X_data, Y_data, X_phys, ...
                                   physicsFcn, physParam, lossWeights);
                    Lvec = double(Lvec(:)).';  % 1 x n_terms

                    obj.lossHistory.iter(end+1)  = obj.iteration;
                    obj.lossHistory.total(end+1) = Lvec(1);

                    if isempty(obj.lossHistory.components)
                        % 第一次: 初始化为 n_terms x 1
                        obj.lossHistory.components = Lvec(:);
                    else
                        % 之后: 在第二维追加一列
                        if size(obj.lossHistory.components,1) ~= numel(Lvec)
                            error('Loss component size mismatch between iterations.');
                        end
                        obj.lossHistory.components(:, end+1) = Lvec(:);
                    end
                else
                    % 如果没实现 evalLossComponents，也至少记录总损失
                    Ltotal = double(extractdata(loss));
                    obj.lossHistory.iter(end+1)  = obj.iteration;
                    obj.lossHistory.total(end+1) = Ltotal;
                    % components 留空
                end

                if mod(ep, 100) == 0
                    fprintf('Epoch %d / %d, Loss = %.3e\n', ep, epochs, extractdata(loss));
                end
            end
        end

        %% 保存模型
        function saveModel(obj, filename)
            save(filename, 'obj');
        end

        %% 抽象损失（子类实现）
        function [loss, grads] = computeLoss(obj, params, ...
                                             X_data, Y_data, X_phys, ...
                                             physicsFcn, physParam, lossWeights)
            %#ok<INUSD>
            error('computeLoss must be implemented in subclass.');
        end
    end

    %% ========== 受保护的方法（基类内部/子类可用） ==========
    methods (Access = protected)
        % 调用子类 computeLoss 的内部接口
        function [loss, grads] = pinnLossInternal(obj, params, ...
                                                 X_data, Y_data, X_phys, ...
                                                 physicsFcn, physParam, lossWeights)
            [loss, grads] = obj.computeLoss(params, ...
                                            X_data, Y_data, X_phys, ...
                                            physicsFcn, physParam, lossWeights);
        end

        % 参数初始化（等价于原 initPINNParams）
        function params = initParams(~, layers)
            num_layers = numel(layers) - 1;
            params = struct();

            for i = 1:num_layers
                in_dim  = layers(i);
                out_dim = layers(i+1);

                limit = sqrt(6/(in_dim + out_dim));   % Xavier uniform

                W = rand(out_dim, in_dim)*2*limit - limit;
                b = zeros(out_dim, 1);

                params.(sprintf('W%d', i)) = dlarray(single(W));
                params.(sprintf('b%d', i)) = dlarray(single(b));
            end
        end

        % Adam 状态初始化（等价于原 initAdamState）
        function [stateAvg, stateAvgSq] = initAdamState(~, params)
            stateAvg   = struct();
            stateAvgSq = struct();

            fn = fieldnames(params);
            for i = 1:numel(fn)
                name = fn{i};

                p = params.(name);
                zeros_like = dlarray(zeros(size(p), 'like', p));

                stateAvg.(name)   = zeros_like;
                stateAvgSq.(name) = zeros_like;
            end
        end
    end

    %% ========== 静态方法（无状态工具函数） ==========
    methods (Static)
        % dlfeval 的统一入口（用于 computeLoss）
        function [loss, grads] = lossWrapper(obj, params, ...
                                             X_data, Y_data, X_phys, ...
                                             physicsFcn, physParam, lossWeights)
            [loss, grads] = obj.pinnLossInternal(params, ...
                                X_data, Y_data, X_phys, ...
                                physicsFcn, physParam, lossWeights);
        end

        % dlfeval 入口（用于 evalLossComponents）
        function Lvec = evalWrapper(obj, params, ...
                                    X_data, Y_data, X_phys, ...
                                    physicsFcn, physParam, lossWeights)
            Lvec = obj.evalLossComponents(params, ...
                                          X_data, Y_data, X_phys, ...
                                          physicsFcn, physParam, lossWeights);
        end

        % 前向传播（等价于原 forwardPINN），可给任意 params
        function Y = forwardWithParams(params, X)
            % 确保 X 是 dlarray(single)
            if ~isa(X, 'dlarray')
                X = dlarray(single(X));
            else
                if ~isa(extractdata(X), 'single')
                    X = dlarray(single(extractdata(X)));
                end
            end

            % 自动统计层数
            num_layers = 0;
            while isfield(params, sprintf('W%d', num_layers + 1))
                num_layers = num_layers + 1;
            end

            A = X;
            % 前 num_layers-1 层 tanh
            for i = 1:(num_layers-1)
                W = params.(sprintf('W%d', i));
                b = params.(sprintf('b%d', i));

                Z = W*A + b;
                A = tanh(Z);
            end

            % 最后一层线性
            W = params.(sprintf('W%d', num_layers));
            b = params.(sprintf('b%d', num_layers));

            Y = W*A + b;
        end

        % ========= 通用工具函数：归一化 / 反归一化 =========
        function [Xn, normInfo] = normalizeData(X, mode, normInfo)
            %NORMALIZEDATA  对列样本数据做归一化（特征在行）
            %
            %   [Xn, normInfo] = BasePINN.normalizeData(X, mode)
            %   [Xn, normInfo] = BasePINN.normalizeData(X, mode, normInfo)
            %
            %   mode: 'none' | 'zscore' | 'minmax'
            %   X   : (d x N) 矩阵，d 维特征，N 个样本
            %
            %   normInfo 结构:
            %     .mode  = mode
            %   对于 'zscore':
            %     .mu    : d x 1
            %     .sigma : d x 1
            %   对于 'minmax':
            %     .xmin  : d x 1
            %     .xmax  : d x 1

            if nargin < 2 || isempty(mode)
                mode = 'none';
            end
            if nargin < 3
                normInfo = struct();
            end

            Xn = X;
            modeLower = lower(mode);

            switch modeLower
                case 'none'
                    normInfo.mode = 'none';

                case 'zscore'
                    if ~isfield(normInfo, 'mu') || ~isfield(normInfo, 'sigma')
                        normInfo.mu    = mean(X, 2);
                        normInfo.sigma = std(X, 0, 2);
                        % 防止除 0
                        normInfo.sigma(normInfo.sigma == 0) = 1;
                    end
                    normInfo.mode = 'zscore';
                    Xn = (X - normInfo.mu) ./ normInfo.sigma;

                case 'minmax'
                    if ~isfield(normInfo, 'xmin') || ~isfield(normInfo, 'xmax')
                        normInfo.xmin = min(X, [], 2);
                        normInfo.xmax = max(X, [], 2);
                    end
                    normInfo.mode = 'minmax';

                    denom = normInfo.xmax - normInfo.xmin;
                    denom(denom == 0) = 1;

                    Xn = (X - normInfo.xmin) ./ denom;

                otherwise
                    error('BasePINN:normalizeData', ...
                          'Unknown normalization mode "%s".', mode);
            end
        end

        function X = denormalizeData(Xn, normInfo)
            %DENORMALIZEDATA  根据 normInfo 做反归一化
            %
            %   X = BasePINN.denormalizeData(Xn, normInfo)

            if ~isfield(normInfo, 'mode') || strcmpi(normInfo.mode, 'none')
                X = Xn;
                return;
            end

            modeLower = lower(normInfo.mode);

            switch modeLower
                case 'zscore'
                    mu    = normInfo.mu;
                    sigma = normInfo.sigma;
                    sigma(sigma == 0) = 1;
                    X = Xn .* sigma + mu;

                case 'minmax'
                    xmin = normInfo.xmin;
                    xmax = normInfo.xmax;
                    denom = xmax - xmin;
                    denom(denom == 0) = 1;
                    X = Xn .* denom + xmin;

                otherwise
                    error('BasePINN:denormalizeData', ...
                          'Unknown normalization mode "%s".', normInfo.mode);
            end
        end

        % ========= 通用工具函数：train / test 切分 =========
        function [Xtrain, Ytrain, Xtest, Ytest, idxTrain, idxTest] = ...
                 trainTestSplit(X, Y, testRatio, doShuffle)
            %TRAINTESTSPLIT  按列切分训练/测试集
            %
            %   [Xtr,Ytr,Xte,Yte,idxTr,idxTe] = BasePINN.trainTestSplit(X, Y, testRatio, doShuffle)
            %
            %   X: d_in x N
            %   Y: d_out x N
            %   testRatio: 测试集比例，默认 0.2
            %   doShuffle: 是否打乱，默认 true

            if nargin < 3 || isempty(testRatio)
                testRatio = 0.2;
            end
            if nargin < 4 || isempty(doShuffle)
                doShuffle = true;
            end

            N = size(X, 2);
            if ~isempty(Y) && size(Y, 2) ~= N
                error('BasePINN:trainTestSplit', ...
                      'X and Y must have the same number of columns (samples).');
            end

            nTest  = max(1, floor(N * testRatio));
            nTrain = N - nTest;

            if doShuffle
                idx = randperm(N);
            else
                idx = 1:N;
            end

            idxTrain = idx(1:nTrain);
            idxTest  = idx(nTrain+1:end);

            Xtrain = X(:, idxTrain);
            Xtest  = X(:, idxTest);

            if isempty(Y)
                Ytrain = [];
                Ytest  = [];
            else
                Ytrain = Y(:, idxTrain);
                Ytest  = Y(:, idxTest);
            end
        end
    end
end
===== ./PINN/HysteresisInversePINN.m =====
classdef HysteresisInversePINN < BasePINN
    % -----------------------------------------------------------
    % Network A: 逆迟滞 PINN (u -> v_pred)
    %
    % 损失包括四项（每项由 lambda 控制，为0则跳过计算）：
    %   1) 数据项        L_data
    %   2) 物理一致性项  L_phys
    %   3) 单调性约束    L_mono
    %   4) 输出平滑项    L_smooth
    %
    % X_data: 用于数据项和单调/平滑项的输入 u
    % X_phys: 用于物理残差 H(v)≈u 的输入 u（可为子采样点）
    % -----------------------------------------------------------

    properties
        physParam       % 正向迟滞模型参数
        lossWeights     % 结构体 {lambdaData, lambdaPhys, lambdaMono, lambdaSmooth}
        physicsFcn      % 正向迟滞算子 (function handle)
    end

    methods
        %% ---------- 构造 ----------
        function obj = HysteresisInversePINN(layers, physParam, lossWeights, physicsFcn)
            obj@BasePINN(layers);
            obj.physParam   = physParam;
            obj.lossWeights = lossWeights;
            obj.physicsFcn  = physicsFcn;
        end

        %% ---------- 核心损失 ----------
        function [loss, grads] = computeLoss(~, params, ...
                                             X_data, Y_data, X_phys, ...
                                             physicsFcn, physParam, lossWeights)

            % -------------------------------
            % 1) 数据前向：v_pred_data = f(u_data)
            % -------------------------------
            v_pred_data = BasePINN.forwardWithParams(params, X_data);

            % -------------------------------
            % 2) 数据项 L_data
            % -------------------------------
            if lossWeights.lambdaData ~= 0
                L_data = mean((v_pred_data - Y_data).^2, "all");
            else
                L_data = dlarray(single(0));
            end

            % -------------------------------
            % 3) 物理一致性项 L_phys: H(v_pred_phys) ≈ u_phys
            %    v_pred_phys 在 X_phys 上前向（可抽样）
            % -------------------------------
            if lossWeights.lambdaPhys ~= 0
                v_pred_phys = BasePINN.forwardWithParams(params, X_phys);
                u_hat = physicsFcn(v_pred_phys, physParam);
                L_phys = mean((u_hat - X_phys).^2, "all");
            else
                L_phys = dlarray(single(0));
            end

            % -------------------------------
            % 4) 单调性约束 L_mono （在 X_data 上 dv/du >= 0）
            % -------------------------------
            if lossWeights.lambdaMono ~= 0
                dv_du = dlgradient(sum(v_pred_data), X_data);
                mono_penalty = relu(-dv_du);      % 只惩罚 dv/du<0 部分
                L_mono = mean(mono_penalty.^2, "all");
            else
                L_mono = dlarray(single(0));
            end

            % -------------------------------
            % 5) 输出平滑项 L_smooth （在 X_data 上）
            % -------------------------------
            if lossWeights.lambdaSmooth ~= 0
                v_seq = v_pred_data;
                if size(v_seq,2) > 1
                    dv = v_seq(:,2:end) - v_seq(:,1:end-1);
                    L_smooth = mean(dv.^2, "all");
                else
                    L_smooth = dlarray(single(0));
                end
            else
                L_smooth = dlarray(single(0));
            end

            % -------------------------------
            % 6) 总损失
            % -------------------------------
            loss = ...
                lossWeights.lambdaData   * L_data   + ...
                lossWeights.lambdaPhys   * L_phys   + ...
                lossWeights.lambdaMono   * L_mono   + ...
                lossWeights.lambdaSmooth * L_smooth;

            % -------------------------------
            % 7) 梯度
            % -------------------------------
            grads = dlgradient(loss, params);
        end

        %% ---------- 拆分各项损失（给 Base / demo 记录用） ----------
        function Lvec = evalLossComponents(~, params, ...
                                           X_data, Y_data, X_phys, ...
                                           physicsFcn, physParam, lossWeights)
            % 与 computeLoss 类似，但返回数值向量:
            % Lvec = [Ltotal, Ldata, Lphys, Lmono, Lsmooth]

            % 1) data forward
            v_pred_data = BasePINN.forwardWithParams(params, X_data);

            % 2) L_data
            if lossWeights.lambdaData ~= 0
                L_data_dl = mean((v_pred_data - Y_data).^2, "all");
                Ldata = double(extractdata(L_data_dl));
            else
                Ldata = 0;
            end

            % 3) L_phys
            if lossWeights.lambdaPhys ~= 0
                v_pred_phys = BasePINN.forwardWithParams(params, X_phys);
                u_hat = physicsFcn(v_pred_phys, physParam);
                L_phys_dl = mean((u_hat - X_phys).^2, "all");
                Lphys = double(extractdata(L_phys_dl));
            else
                Lphys = 0;
            end

            % 4) L_mono
            if lossWeights.lambdaMono ~= 0
                dv_du = dlgradient(sum(v_pred_data), X_data);
                mono_penalty = relu(-dv_du);
                L_mono_dl = mean(mono_penalty.^2, "all");
                Lmono = double(extractdata(L_mono_dl));
            else
                Lmono = 0;
            end

            % 5) L_smooth
            if lossWeights.lambdaSmooth ~= 0 && size(v_pred_data,2) > 1
                dv = v_pred_data(:,2:end) - v_pred_data(:,1:end-1);
                L_smooth_dl = mean(dv.^2, "all");
                Lsmooth = double(extractdata(L_smooth_dl));
            else
                Lsmooth = 0;
            end

            % 6) 总损失（按 lambda 加权）
            Ltotal = ...
                lossWeights.lambdaData   * Ldata   + ...
                lossWeights.lambdaPhys   * Lphys   + ...
                lossWeights.lambdaMono   * Lmono   + ...
                lossWeights.lambdaSmooth * Lsmooth;

            Lvec = [Ltotal, Ldata, Lphys, Lmono, Lsmooth];
        end
    end
end
