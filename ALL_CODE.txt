===== ./demo_hysteresis_inverse_PINN_staged_dynamic.m =====
%% demo_hysteresis_inverse_PINN_staged_plot.m
% 基于 BasePINN + HysteresisInversePINN 的分阶段训练 demo
% - 每阶段训练后画 train/test 拟合 + loss
% - 训练结束后画总的拟合 + loss + 滞回曲线
% - Stage 2 / Stage 3 使用“分段训练 + 每段前动态重采样物理点”

clear; clc; close all;
rng(0);

%% ===== 0. 一些训练超参数 =====
% 时间与采样
dt      = 1e-3;
T_end   = 1.2;
t       = (0:dt:T_end-dt).';
N       = numel(t);

train_ratio = 0.8;
N_train = floor(N * train_ratio);
t_train = t(1:N_train);
t_test  = t(N_train+1:end);

% 各阶段下采样 / 物理点抽样步长（原始设定）
data_stride_stage1 = 5;
data_stride_stage2 = 2;
phys_stride_stage2 = 5;
phys_stride_stage3 = 5;

% 分阶段训练轮数 / 学习率
epochs1 = 2000; lr1 = 1e-3;   % Data only
epochs2 = 3000; lr2 = 1e-3;   % Data + Phys
epochs3 = 1000; lr3 = 2e-4;   % Full loss

% 动态物理点重采样的“分段长度”
%   Stage 2: 每 100 epoch 重采一次物理点
%   Stage 3: 每 200 epoch 重采一次物理点
chunk_stage2 = 100;
chunk_stage3 = 200;

%% ===== 1. 生成全序列数据 (GPI 正向迟滞) =====
% 逆模型输入：u_all（位移），输出：v_all（电压）
v_all = 5 + 4*sin(2*pi*1*t) + 1.5*sin(2*pi*3*t);
v_all = max(0, min(10, v_all));   % 限幅 [0,10]

u_all = zeros(size(v_all));
Fr = [];
for k = 1:N
    [u_all(k), Fr] = GPI_Model(v_all(k), Fr);   % 这里调用你已有的 GPI_Model
end

u_train_full = u_all(1:N_train);
v_train_full = v_all(1:N_train);
u_test_full  = u_all(N_train+1:end);
v_test_full  = v_all(N_train+1:end);

%% ===== 1.2 归一化 (z-score) =====
% 注意：网络在“归一化空间”里工作：u_norm -> v_norm
%       画图/MSE 时再反归一化回物理量

% 训练集归一化
[u_train_norm_row, normIn]  = BasePINN.normalizeData(u_train_full.', 'zscore');
[v_train_norm_row, normOut] = BasePINN.normalizeData(v_train_full.', 'zscore');

% 测试集用同一个 normInfo 做变换（保持一致）
[u_test_norm_row, ~]  = BasePINN.normalizeData(u_test_full.',  'zscore', normIn);
[v_test_norm_row, ~]  = BasePINN.normalizeData(v_test_full.',  'zscore', normOut);

% 方便后面直接用的 dlarray 形式（归一化空间）
X_test_dl = dlarray(single(u_test_norm_row));
Y_test_dl = dlarray(single(v_test_norm_row));

%% ===== 2. 正向迟滞参数（给 physicsFcn 用） =====
physParam.rd  = [-8 -6 -4 -2 0 2 4 6 8];
physParam.eta = [1 1 1 1 1.2437 -0.1103 -0.0787 -0.1742 -0.1767];
physParam.r   = [0 1 2 3 4 5];
physParam.P   = [0.7460 0.1860 0.0650 0.0855 0.0362 -0.1159];

%% ===== 3. 定义网络 =====
layers = [1 64 64 1];
dummyWeights = struct('lambdaData',1,'lambdaPhys',0,'lambdaMono',0,'lambdaSmooth',0);
pinn = HysteresisInversePINN(layers, physParam, dummyWeights, @physicsFcn_GPI_forward);

% 设置归一化参数（供 computeLoss / 其他方法内部使用）
% 假定你在 HysteresisInversePINN 里已经实现了 setNormalization / applyDenormOut 等
pinn.setNormalization(normIn, normOut);

%% 工具：记录各阶段 lossHistory 范围
stageRanges = struct('name',{},'idx',{});

%% ===== 4. Stage 1: Data only + 强下采样 =====
disp("===== Stage 1: Data only =====");

idx_s1 = 1:data_stride_stage1:N_train;

% 训练数据在归一化空间：u_norm -> v_norm
X_s1_dl = dlarray(single(u_train_norm_row(idx_s1)));
Y_s1_dl = dlarray(single(v_train_norm_row(idx_s1)));

lossW1.lambdaData   = 1;
lossW1.lambdaPhys   = 0;
lossW1.lambdaMono   = 0;
lossW1.lambdaSmooth = 0;

prevLen = numel(pinn.lossHistory.total);
pinn.train(X_s1_dl, Y_s1_dl, X_s1_dl, ...
           @physicsFcn_GPI_forward, physParam, lossW1, ...
           epochs1, lr1);
newLen = numel(pinn.lossHistory.total);
stageRanges(end+1).name = 'Stage 1'; 
stageRanges(end).idx    = prevLen+1:newLen;

% --- Stage 1 绘图 ---
plot_stage_fit(pinn, 1, t_train, u_train_full, v_train_full, ...
               t_test, u_test_full, v_test_full, ...
               normIn, normOut);
plot_stage_loss(pinn, 1, stageRanges(end).idx);

%% ===== 5. Stage 2: Data + Physics，中等下采样 + 动态物理点抽样 =====
disp("===== Stage 2: Data + Physics (dynamic collocation) =====");

idx_s2 = 1:data_stride_stage2:N_train;

X_s2_dl = dlarray(single(u_train_norm_row(idx_s2)));
Y_s2_dl = dlarray(single(v_train_norm_row(idx_s2)));

% 物理点数量保持和原来大致一致：ceil(N_s2 / phys_stride_stage2)
N_s2     = numel(idx_s2);
N_phys2  = ceil(N_s2 / phys_stride_stage2);

lossW2.lambdaData   = 1;
lossW2.lambdaPhys   = 10;
lossW2.lambdaMono   = 0;
lossW2.lambdaSmooth = 0;

prevLen = numel(pinn.lossHistory.total);

% ---- 核心改动：把 epochs2 拆成多个“小段”，每一段前重采物理点 ----
remaining = epochs2;
while remaining > 0
    thisEpochs = min(chunk_stage2, remaining);

    % 基于当前网络状态动态采样物理点（hybrid = random + residual top-K）
    X_phys_s2_dl = samplePhysicsPoints(pinn, ...
        u_train_full, u_train_norm_row, ...
        physParam, N_phys2, 'hybrid');  % 'random' | 'residual' | 'hybrid'

    % 在这一小段内，物理点固定，调用内部的 train 做 thisEpochs 轮更新
    fprintf("Segment %d/%d ", (epochs2 - remaining)/chunk_stage2 + 1, epochs2/chunk_stage2);
    pinn.train(X_s2_dl, Y_s2_dl, X_phys_s2_dl, ...
               @physicsFcn_GPI_forward, physParam, lossW2, ...
               thisEpochs, lr2);

    remaining = remaining - thisEpochs;
end

newLen = numel(pinn.lossHistory.total);
stageRanges(end+1).name = 'Stage 2'; 
stageRanges(end).idx    = prevLen+1:newLen;

% --- Stage 2 绘图 ---
plot_stage_fit(pinn, 2, t_train, u_train_full, v_train_full, ...
               t_test, u_test_full, v_test_full, ...
               normIn, normOut);
plot_stage_loss(pinn, 2, stageRanges(end).idx);

%% ===== 6. Stage 3: Data + Physics + Mono + Smooth，动态物理点采样 =====
disp("===== Stage 3: Full loss (dynamic collocation) =====");

idx_s3 = idx_s2;          % 这里用和 Stage2 相同的下采样

X_s3_dl = dlarray(single(u_train_norm_row(idx_s3)));   % 输入 u_norm
Y_s3_dl = dlarray(single(v_train_norm_row(idx_s3)));   % 输出 v_norm

N_s3     = numel(idx_s3);
N_phys3  = ceil(N_s3 / phys_stride_stage3);

lossW3.lambdaData   = 5;
lossW3.lambdaPhys   = 5;
lossW3.lambdaMono   = 0.1;
lossW3.lambdaSmooth = 0.001;

prevLen = numel(pinn.lossHistory.total);

remaining = epochs3;
while remaining > 0
    thisEpochs = min(chunk_stage3, remaining);

    % 每个“训练小段”开始前，根据当前残差重新采样物理点
    X_phys_s3_dl = samplePhysicsPoints(pinn, ...
        u_train_full, u_train_norm_row, ...
        physParam, N_phys3, 'hybrid');

    fprintf("Segment %d/%d ", (epochs3 - remaining)/chunk_stage3 + 1, epochs3/chunk_stage3);
    pinn.train(X_s3_dl, Y_s3_dl, X_phys_s3_dl, ...
               @physicsFcn_GPI_forward, physParam, lossW3, ...
               thisEpochs, lr3);

    remaining = remaining - thisEpochs;
end

newLen = numel(pinn.lossHistory.total);
stageRanges(end+1).name = 'Stage 3'; 
stageRanges(end).idx    = prevLen+1:newLen;

% --- Stage 3 绘图 ---
plot_stage_fit(pinn, 3, t_train, u_train_full, v_train_full, ...
               t_test, u_test_full, v_test_full, ...
               normIn, normOut);
plot_stage_loss(pinn, 3, stageRanges(end).idx);

%% ===== 7. 训练结束后：总拟合 + 总 loss + 滞回曲线 =====
disp("===== Final plots =====");

% ---- 总拟合：train + test ----
% 1) 把全量 u_train / u_test 变成归一化空间
[u_train_norm_full_row, ~] = BasePINN.normalizeData(u_train_full.', normIn.mode, normIn);
[u_test_norm_full_row,  ~] = BasePINN.normalizeData(u_test_full.',  normIn.mode, normIn);

% 2) 网络前向：u_norm -> v_norm
v_train_pred_norm_dl = pinn.forward(dlarray(single(u_train_norm_full_row)));
v_test_pred_norm_dl  = pinn.forward(dlarray(single(u_test_norm_full_row)));

v_train_pred_norm_row = double(extractdata(v_train_pred_norm_dl));
v_test_pred_norm_row  = double(extractdata(v_test_pred_norm_dl));

% 3) 反归一化到物理电压
v_train_pred_row = BasePINN.denormalizeData(v_train_pred_norm_row, normOut);
v_test_pred_row  = BasePINN.denormalizeData(v_test_pred_norm_row,  normOut);

v_train_pred = v_train_pred_row.';   % N_train x 1
v_test_pred  = v_test_pred_row.';    % N_test x 1

mse_train = mean((v_train_pred - v_train_full).^2);
mse_test  = mean((v_test_pred  - v_test_full).^2);

figure('Name','Final Fit (Train + Test)');
subplot(2,1,1); hold on; box on;
plot(t_train, v_train_full, 'b', 'LineWidth',1.2);
plot(t_train, v_train_pred, 'r--','LineWidth',1.2);
xlabel('t [s]'); ylabel('Voltage v [V]');
title(sprintf('Train fit (MSE=%.2e)', mse_train));
legend('True v','PINN v_{pred}','Location','best');

subplot(2,1,2); hold on; box on;
plot(t_test, v_test_full, 'b', 'LineWidth',1.2);
plot(t_test, v_test_pred, 'r--','LineWidth',1.2);
xlabel('t [s]'); ylabel('Voltage v [V]');
title(sprintf('Test fit (MSE=%.2e)', mse_test));
legend('True v','PINN v_{pred}','Location','best');

% ---- 总 loss 曲线 ----
if ~isempty(pinn.lossHistory.components)
    it_all = pinn.lossHistory.iter;
    Lall   = pinn.lossHistory.components;   % 5 x K: [total,data,phys,mono,smooth]
    Ltotal = Lall(1,:);
    Ldata  = Lall(2,:);
    Lphys  = Lall(3,:);
    Lmono  = Lall(4,:);
    Lsmooth= Lall(5,:);

    figure('Name','Total Loss History');
    hold on; box on;
    plot(it_all, Ltotal, 'k','LineWidth',1.2);
    plot(it_all, Ldata,  'b--','LineWidth',1.0);
    plot(it_all, Lphys,  'r--','LineWidth',1.0);
    plot(it_all, Lmono,  'g--','LineWidth',1.0);
    plot(it_all, Lsmooth,'m--','LineWidth',1.0);
    xlabel('Iteration'); ylabel('Loss');
    legend('Total','Data','Phys','Mono','Smooth','Location','best');
    title('Overall Loss History');
    grid on;
end

% ---- 滞回曲线：真实 (v_all,u_all) vs PINN 逆 + 正向 GPI ----
% 1) 全序列 u_all -> 归一化 u_norm_all
[u_all_norm_row, ~] = BasePINN.normalizeData(u_all.', normIn.mode, normIn);

% 2) 逆模型：u_norm_all -> v_norm_all
v_pred_all_norm_dl  = pinn.forward(dlarray(single(u_all_norm_row)));
v_pred_all_norm_row = double(extractdata(v_pred_all_norm_dl));

% 3) 反归一化到物理电压
v_pred_all_row = BasePINN.denormalizeData(v_pred_all_norm_row, normOut);
v_pred_all     = v_pred_all_row.';   % N x 1

% 4) 正向 GPI：v_pred_all -> u_hat_all
u_hat_all_dl = physicsFcn_GPI_forward(dlarray(single(v_pred_all.')), physParam);
u_hat_all    = double(extractdata(u_hat_all_dl)).';

figure('Name','Hysteresis Loop');
hold on; box on;
plot(v_all,       u_all,    'b.', 'DisplayName','True hysteresis');
plot(v_pred_all,  u_hat_all,'r.', 'DisplayName','PINN inverse -> forward');
xlabel('Voltage v'); ylabel('Displacement u');
legend('Location','best');
title('Hysteresis loop comparison');
grid on;

%% ================== 辅助绘图函数 ==================
function plot_stage_fit(pinn, stageId, t_train, u_train, v_train, ...
                        t_test,  u_test,  v_test, ...
                        normIn, normOut)
    % 1) 物理 u -> 归一化 u_norm
    [u_train_norm_row, ~] = BasePINN.normalizeData(u_train.', normIn.mode, normIn);
    [u_test_norm_row,  ~] = BasePINN.normalizeData(u_test.',  normIn.mode, normIn);

    % 2) 前向预测（归一化空间）
    v_train_pred_norm_dl = pinn.forward(dlarray(single(u_train_norm_row)));
    v_test_pred_norm_dl  = pinn.forward(dlarray(single(u_test_norm_row)));

    v_train_pred_norm_row = double(extractdata(v_train_pred_norm_dl));
    v_test_pred_norm_row  = double(extractdata(v_test_pred_norm_dl));

    % 3) 反归一化回物理电压
    v_train_pred_row = BasePINN.denormalizeData(v_train_pred_norm_row, normOut);
    v_test_pred_row  = BasePINN.denormalizeData(v_test_pred_norm_row,  normOut);

    v_train_pred = v_train_pred_row.';  % N_train x 1
    v_test_pred  = v_test_pred_row.';   % N_test x 1

    mse_train = mean((v_train_pred - v_train).^2);
    mse_test  = mean((v_test_pred  - v_test).^2);

    figure('Name',sprintf('Stage %d Fit', stageId));
    subplot(2,1,1); hold on; box on;
    plot(t_train, v_train, 'b','LineWidth',1.2);
    plot(t_train, v_train_pred,'r--','LineWidth',1.2);
    xlabel('t [s]'); ylabel('Voltage v [V]');
    title(sprintf('Stage %d Train fit (MSE=%.2e)', stageId, mse_train));
    legend('True v','PINN v_{pred}','Location','best');

    subplot(2,1,2); hold on; box on;
    plot(t_test, v_test, 'b','LineWidth',1.2);
    plot(t_test, v_test_pred,'r--','LineWidth',1.2);
    xlabel('t [s]'); ylabel('Voltage v [V]');
    title(sprintf('Stage %d Test fit (MSE=%.2e)', stageId, mse_test));
    legend('True v','PINN v_{pred}','Location','best');
end

function plot_stage_loss(pinn, stageId, idxRange)
    if isempty(pinn.lossHistory.components)
        return;
    end
    it   = pinn.lossHistory.iter(idxRange);
    Lall = pinn.lossHistory.components(:, idxRange);
    Ltotal = Lall(1,:);
    Ldata  = Lall(2,:);
    Lphys  = Lall(3,:);
    Lmono  = Lall(4,:);
    Lsmooth= Lall(5,:);

    figure('Name',sprintf('Stage %d Loss', stageId));
    hold on; box on;
    plot(it, Ltotal, 'k','LineWidth',1.2);
    plot(it, Ldata,  'b--','LineWidth',1.0);
    plot(it, Lphys,  'r--','LineWidth',1.0);
    plot(it, Lmono,  'g--','LineWidth',1.0);
    plot(it, Lsmooth,'m--','LineWidth',1.0);
    xlabel('Iteration'); ylabel('Loss');
    legend('Total','Data','Phys','Mono','Smooth','Location','best');
    title(sprintf('Stage %d Loss History', stageId));
    grid on;
end
===== ./GPI_Model.m =====
function [y,Fr1_updated] = GPI_Model(v, Fr1_previous)
    %% v  % 0-10V,0.001s,1000hz
    
    rd = [-8    -6    -4    -2     0     2     4     6     8];
    eta = [1.0000    1.0000    1.0000    1.0000    1.2437   -0.1103   -0.0787   -0.1742   -0.1767];
    r=[0     1     2     3     4     5];
    n=length(r);
    P=[0.7460    0.1860    0.0650    0.0855    0.0362   -0.1159];

    Ad = zeros(length(rd),1);
    for z = 1:length(rd)
        if rd(z) > 0
            Ad(z) = max(v - rd(z), 0);
        elseif rd(z) < 0
            Ad(z) = min(v - rd(z), 0);
        else
            Ad(z) = v;
        end
    end
    w = eta * Ad;

    Fr1 = zeros(n, 1);
    % 对于每个阈值r(j)，更新Fr1(j)
    for j = 1:n
        if ~isempty(Fr1_previous)
            Fr1(j) = Fr1_previous(j);
        end
        F_inc = w;
        F_dec = w;
        A = F_inc - r(j);
        B = min(F_dec + r(j), Fr1(j));
        Fr1(j) = max(A, B); % Equation (1)
    end
    %% 输出y
    y = P*Fr1;
    Fr1_updated = Fr1;
end
===== ./optimizers/samplePhysicsPoints.m =====
%% ================== 动态物理点采样函数 ==================
function X_phys_dl = samplePhysicsPoints(pinn, ...
    u_train_full, u_train_norm_row, ...
    physParam, N_phys, mode)
% 动态采样物理点
%  - pinn: 已训练到当前阶段的 PINN 对象
%  - u_train_full      : N_train x 1 (物理量)
%  - u_train_norm_row  : 1 x N_train (归一化空间)
%  - N_phys            : 本次要选多少个 collocation 点
%  - mode              : 'random' | 'residual' | 'hybrid'

    if nargin < 6 || isempty(mode)
        mode = 'hybrid';
    end

    N_train = numel(u_train_full);

    switch lower(mode)
        case 'random'
            idx_phys = randperm(N_train, min(N_phys, N_train));

        case 'residual'
            % 使用当前网络计算全局物理残差 |H(v_pred) - u|
            [~, idx_phys] = residualTopK(pinn, u_train_full, u_train_norm_row, ...
                                         physParam, N_phys);

        otherwise % 'hybrid'：一半随机，一半残差大的
            N1 = floor(N_phys/2);
            N2 = N_phys - N1;

            idx_rand = randperm(N_train, min(N1, N_train));
            [~, idx_top] = residualTopK(pinn, u_train_full, u_train_norm_row, ...
                                        physParam, min(N2, N_train));

            idx_phys = unique([idx_rand(:); idx_top(:)]).';
    end

    % 取出对应的归一化 u_norm 作为 X_phys
    u_phys_norm_seg = u_train_norm_row(:, idx_phys);  % 1 x N_phys
    X_phys_dl = dlarray(single(u_phys_norm_seg));
end

function [e_sorted, idx_top] = residualTopK(pinn, u_train_full, u_train_norm_row, ...
                                            physParam, K)
    % 1) 在所有训练点上预测 v（归一化空间）
    u_norm_dl = dlarray(single(u_train_norm_row));
    v_norm_dl = pinn.forward(u_norm_dl);
    v_norm    = double(extractdata(v_norm_dl));   % 1 x N

    % 2) 反归一化到物理空间 v_phys
    % 这里假定 pinn 内部有 applyDenormOut，如果没有可以改成:
    %   v_phys_row = BasePINN.denormalizeData(v_norm, pinn.normOut);
    v_phys_row = pinn.applyDenormOut(v_norm);
    v_phys     = v_phys_row.';                    % N x 1

    % 3) 正向迟滞模型
    u_hat_dl = physicsFcn_GPI_forward(dlarray(single(v_phys.')), physParam);
    u_hat    = double(extractdata(u_hat_dl)).';   % N x 1

    % 4) 残差
    e = abs(u_hat - u_train_full);               % N x 1
    [e_sorted, idx_sorted] = sort(e, 'descend');
    K = min(K, numel(e_sorted));
    idx_top = idx_sorted(1:K);
end===== ./physicsFcn_GPI_forward.m =====
%% ===== 正向迟滞算子（GPI）：dlarray 版本 =====
function u_hat = physicsFcn_GPI_forward(v_pred, physParam)
    rd  = physParam.rd;
    eta = physParam.eta;
    r   = physParam.r;
    P   = physParam.P;

    if ~isa(v_pred, 'dlarray')
        v_pred = dlarray(single(v_pred));
    else
        if ~isa(extractdata(v_pred), 'single')
            v_pred = dlarray(single(extractdata(v_pred)));
        end
    end

    N  = size(v_pred, 2);
    nR = numel(r);
    nD = numel(rd);

    Fr1_prev = zeros(nR, 1, 'single');
    u_hat_data = zeros(1, N, 'single');

    for k = 1:N
        v = v_pred(1,k);

        % Dead-zone
        Ad = zeros(nD,1,'single');
        for z = 1:nD
            if rd(z) > 0
                Ad(z) = max(v - rd(z), 0);
            elseif rd(z) < 0
                Ad(z) = min(v - rd(z), 0);
            else
                Ad(z) = v;
            end
        end
        w = eta * Ad;

        % Play operator
        Fr1 = zeros(nR,1,'single');
        for j = 1:nR
            A = w - r(j);
            B = min(w + r(j), Fr1_prev(j));
            Fr1(j) = max(A, B);
        end

        Fr1_prev = Fr1;
        u_hat_data(1,k) = P * Fr1;
    end

    u_hat = dlarray(u_hat_data);
end===== ./PINN/BasePINN.m =====
classdef BasePINN < handle
    %BASEPINN Generic PINN base class.
    %   - 管理参数初始化
    %   - 前向传播 (MLP)
    %   - Adam 训练 + 热启动
    %   - dlfeval 通过静态 wrapper 统一调用
    %
    % 子类必须实现:
    %   [loss, grads] = computeLoss(obj, params, X_data, Y_data, X_phys, physicsFcn, physParam, lossWeights)

    properties
        params          % W1,b1,...,WL,bL
        layers
        lossHistory     % 通用损失记录: struct('iter',[],'total',[],'components',[])
        iteration       % Adam 迭代计数（支持热启动）
        adamAvg
        adamAvgSq
    end

    methods
        %% 构造函数
        function obj = BasePINN(layers)
            obj.layers = layers;
            obj.params = obj.initParams(layers);
            [obj.adamAvg, obj.adamAvgSq] = obj.initAdamState(obj.params);

            % 通用的 loss 记录结构
            obj.lossHistory = struct( ...
                'iter',      [], ...   % 1 x K
                'total',     [], ...   % 1 x K
                'components', [] );    % n_terms x K

            obj.iteration = 0;
        end

        %% 前向预测，使用当前 obj.params
        function Y = forward(obj, X)
            Y = BasePINN.forwardWithParams(obj.params, X);
        end

        %% 训练（Adam），支持热启动
        function obj = train(obj, X_data, Y_data, X_phys, ...
                             physicsFcn, physParam, lossWeights, ...
                             epochs, lr)

            for ep = 1:epochs
                obj.iteration = obj.iteration + 1;

                % 通过静态 wrapper + dlfeval 调用 computeLoss
                [loss, grads] = dlfeval(@BasePINN.lossWrapper, ...
                    obj, obj.params, ...
                    X_data, Y_data, X_phys, ...
                    physicsFcn, physParam, lossWeights);

                % Adam 更新（逻辑和原 trainPINN 完全一致）
                fn = fieldnames(obj.params);
                for i = 1:numel(fn)
                    name = fn{i};

                    [obj.params.(name), ...
                     obj.adamAvg.(name), ...
                     obj.adamAvgSq.(name)] = ...
                        adamupdate( ...
                            obj.params.(name), grads.(name), ...
                            obj.adamAvg.(name), obj.adamAvgSq.(name), ...
                            obj.iteration, lr);
                end

                % ===== 通用记录损失（如果子类实现了 evalLossComponents）=====
                if ismethod(obj, 'evalLossComponents')
                    % 返回一个向量: [Ltotal, L1, L2, ...]
                    Lvec = dlfeval(@BasePINN.evalWrapper, ...
                                   obj, obj.params, ...
                                   X_data, Y_data, X_phys, ...
                                   physicsFcn, physParam, lossWeights);
                    Lvec = double(Lvec(:)).';  % 1 x n_terms

                    obj.lossHistory.iter(end+1)  = obj.iteration;
                    obj.lossHistory.total(end+1) = Lvec(1);

                    if isempty(obj.lossHistory.components)
                        % 第一次: 初始化为 n_terms x 1
                        obj.lossHistory.components = Lvec(:);
                    else
                        % 之后: 在第二维追加一列
                        if size(obj.lossHistory.components,1) ~= numel(Lvec)
                            error('Loss component size mismatch between iterations.');
                        end
                        obj.lossHistory.components(:, end+1) = Lvec(:);
                    end
                else
                    % 如果没实现 evalLossComponents，也至少记录总损失
                    Ltotal = double(extractdata(loss));
                    obj.lossHistory.iter(end+1)  = obj.iteration;
                    obj.lossHistory.total(end+1) = Ltotal;
                    % components 留空
                end

                if mod(ep, 100) == 0
                    fprintf('Epoch %d / %d, Loss = %.3e\n', ep, epochs, extractdata(loss));
                end
            end
        end

        %% 保存模型
        function saveModel(obj, filename)
            save(filename, 'obj');
        end

        %% 抽象损失（子类实现）
        function [loss, grads] = computeLoss(obj, params, ...
                                             X_data, Y_data, X_phys, ...
                                             physicsFcn, physParam, lossWeights)
            %#ok<INUSD>
            error('computeLoss must be implemented in subclass.');
        end
    end

    %% ========== 受保护的方法（基类内部/子类可用） ==========
    methods (Access = protected)
        % 调用子类 computeLoss 的内部接口
        function [loss, grads] = pinnLossInternal(obj, params, ...
                                                 X_data, Y_data, X_phys, ...
                                                 physicsFcn, physParam, lossWeights)
            [loss, grads] = obj.computeLoss(params, ...
                                            X_data, Y_data, X_phys, ...
                                            physicsFcn, physParam, lossWeights);
        end

        % 参数初始化（等价于原 initPINNParams）
        function params = initParams(~, layers)
            num_layers = numel(layers) - 1;
            params = struct();

            for i = 1:num_layers
                in_dim  = layers(i);
                out_dim = layers(i+1);

                limit = sqrt(6/(in_dim + out_dim));   % Xavier uniform

                W = rand(out_dim, in_dim)*2*limit - limit;
                b = zeros(out_dim, 1);

                params.(sprintf('W%d', i)) = dlarray(single(W));
                params.(sprintf('b%d', i)) = dlarray(single(b));
            end
        end

        % Adam 状态初始化（等价于原 initAdamState）
        function [stateAvg, stateAvgSq] = initAdamState(~, params)
            stateAvg   = struct();
            stateAvgSq = struct();

            fn = fieldnames(params);
            for i = 1:numel(fn)
                name = fn{i};

                p = params.(name);
                zeros_like = dlarray(zeros(size(p), 'like', p));

                stateAvg.(name)   = zeros_like;
                stateAvgSq.(name) = zeros_like;
            end
        end
    end

    %% ========== 静态方法（无状态工具函数） ==========
    methods (Static)
        % dlfeval 的统一入口（用于 computeLoss）
        function [loss, grads] = lossWrapper(obj, params, ...
                                             X_data, Y_data, X_phys, ...
                                             physicsFcn, physParam, lossWeights)
            [loss, grads] = obj.pinnLossInternal(params, ...
                                X_data, Y_data, X_phys, ...
                                physicsFcn, physParam, lossWeights);
        end

        % dlfeval 入口（用于 evalLossComponents）
        function Lvec = evalWrapper(obj, params, ...
                                    X_data, Y_data, X_phys, ...
                                    physicsFcn, physParam, lossWeights)
            Lvec = obj.evalLossComponents(params, ...
                                          X_data, Y_data, X_phys, ...
                                          physicsFcn, physParam, lossWeights);
        end

        % 前向传播（等价于原 forwardPINN），可给任意 params
        function Y = forwardWithParams(params, X)
            % 确保 X 是 dlarray(single)
            if ~isa(X, 'dlarray')
                X = dlarray(single(X));
            else
                if ~isa(extractdata(X), 'single')
                    X = dlarray(single(extractdata(X)));
                end
            end

            % 自动统计层数
            num_layers = 0;
            while isfield(params, sprintf('W%d', num_layers + 1))
                num_layers = num_layers + 1;
            end

            A = X;
            % 前 num_layers-1 层 tanh
            for i = 1:(num_layers-1)
                W = params.(sprintf('W%d', i));
                b = params.(sprintf('b%d', i));

                Z = W*A + b;
                A = tanh(Z);
            end

            % 最后一层线性
            W = params.(sprintf('W%d', num_layers));
            b = params.(sprintf('b%d', num_layers));

            Y = W*A + b;
        end

        % ========= 通用工具函数：归一化 / 反归一化 =========
        function [Xn, normInfo] = normalizeData(X, mode, normInfo)
            %NORMALIZEDATA  对列样本数据做归一化（特征在行）
            %
            %   [Xn, normInfo] = BasePINN.normalizeData(X, mode)
            %   [Xn, normInfo] = BasePINN.normalizeData(X, mode, normInfo)
            %
            %   mode: 'none' | 'zscore' | 'minmax'
            %   X   : (d x N) 矩阵，d 维特征，N 个样本
            %
            %   normInfo 结构:
            %     .mode  = mode
            %   对于 'zscore':
            %     .mu    : d x 1
            %     .sigma : d x 1
            %   对于 'minmax':
            %     .xmin  : d x 1
            %     .xmax  : d x 1

            if nargin < 2 || isempty(mode)
                mode = 'none';
            end
            if nargin < 3
                normInfo = struct();
            end

            Xn = X;
            modeLower = lower(mode);

            switch modeLower
                case 'none'
                    normInfo.mode = 'none';

                case 'zscore'
                    if ~isfield(normInfo, 'mu') || ~isfield(normInfo, 'sigma')
                        normInfo.mu    = mean(X, 2);
                        normInfo.sigma = std(X, 0, 2);
                        % 防止除 0
                        normInfo.sigma(normInfo.sigma == 0) = 1;
                    end
                    normInfo.mode = 'zscore';
                    Xn = (X - normInfo.mu) ./ normInfo.sigma;

                case 'minmax'
                    if ~isfield(normInfo, 'xmin') || ~isfield(normInfo, 'xmax')
                        normInfo.xmin = min(X, [], 2);
                        normInfo.xmax = max(X, [], 2);
                    end
                    normInfo.mode = 'minmax';

                    denom = normInfo.xmax - normInfo.xmin;
                    denom(denom == 0) = 1;

                    Xn = (X - normInfo.xmin) ./ denom;

                otherwise
                    error('BasePINN:normalizeData', ...
                          'Unknown normalization mode "%s".', mode);
            end
        end

        function X = denormalizeData(Xn, normInfo)
            %DENORMALIZEDATA  根据 normInfo 做反归一化
            %
            %   X = BasePINN.denormalizeData(Xn, normInfo)

            if ~isfield(normInfo, 'mode') || strcmpi(normInfo.mode, 'none')
                X = Xn;
                return;
            end

            modeLower = lower(normInfo.mode);

            switch modeLower
                case 'zscore'
                    mu    = normInfo.mu;
                    sigma = normInfo.sigma;
                    sigma(sigma == 0) = 1;
                    X = Xn .* sigma + mu;

                case 'minmax'
                    xmin = normInfo.xmin;
                    xmax = normInfo.xmax;
                    denom = xmax - xmin;
                    denom(denom == 0) = 1;
                    X = Xn .* denom + xmin;

                otherwise
                    error('BasePINN:denormalizeData', ...
                          'Unknown normalization mode "%s".', normInfo.mode);
            end
        end

        % ========= 通用工具函数：train / test 切分 =========
        function [Xtrain, Ytrain, Xtest, Ytest, idxTrain, idxTest] = ...
                 trainTestSplit(X, Y, testRatio, doShuffle)
            %TRAINTESTSPLIT  按列切分训练/测试集
            %
            %   [Xtr,Ytr,Xte,Yte,idxTr,idxTe] = BasePINN.trainTestSplit(X, Y, testRatio, doShuffle)
            %
            %   X: d_in x N
            %   Y: d_out x N
            %   testRatio: 测试集比例，默认 0.2
            %   doShuffle: 是否打乱，默认 true

            if nargin < 3 || isempty(testRatio)
                testRatio = 0.2;
            end
            if nargin < 4 || isempty(doShuffle)
                doShuffle = true;
            end

            N = size(X, 2);
            if ~isempty(Y) && size(Y, 2) ~= N
                error('BasePINN:trainTestSplit', ...
                      'X and Y must have the same number of columns (samples).');
            end

            nTest  = max(1, floor(N * testRatio));
            nTrain = N - nTest;

            if doShuffle
                idx = randperm(N);
            else
                idx = 1:N;
            end

            idxTrain = idx(1:nTrain);
            idxTest  = idx(nTrain+1:end);

            Xtrain = X(:, idxTrain);
            Xtest  = X(:, idxTest);

            if isempty(Y)
                Ytrain = [];
                Ytest  = [];
            else
                Ytrain = Y(:, idxTrain);
                Ytest  = Y(:, idxTest);
            end
        end
    end
end
===== ./PINN/HysteresisInversePINN.m =====
classdef HysteresisInversePINN < BasePINN
    % -----------------------------------------------------------
    % Network A: 逆迟滞 PINN (u -> v_pred)
    %
    % 损失包括四项（每项由 lambda 控制，为0则跳过计算）：
    %   1) 数据项        L_data
    %   2) 物理一致性项  L_phys
    %   3) 单调性约束    L_mono
    %   4) 输出平滑项    L_smooth
    %
    % X_data: 用于数据项和单调/平滑项的输入 (归一化后的 u_norm)
    % Y_data: 数据项目标输出 (归一化后的 v_norm)
    % X_phys: 用于物理残差 H(v)≈u 的输入 (归一化后的 u_norm_phys)
    %
    % 归一化:
    %   - 网络内部全部在归一化空间上工作 (u_norm, v_norm)
    %   - 物理项: 先把 u_norm, v_norm 反归一化为物理量 u_phys, v_phys
    %             再喂 physicsFcn(v_phys, physParam)
    % -----------------------------------------------------------

    properties
        physParam       % 正向迟滞模型参数
        lossWeights     % 结构体 {lambdaData, lambdaPhys, lambdaMono, lambdaSmooth}
        physicsFcn      % 正向迟滞算子 (function handle)

        normIn          % 输入归一化参数 struct，字段至少包含 .mode
        normOut         % 输出归一化参数 struct，字段至少包含 .mode
    end

    methods
        %% ---------- 构造 ----------
        function obj = HysteresisInversePINN(layers, physParam, lossWeights, physicsFcn)
            obj@BasePINN(layers);
            obj.physParam   = physParam;
            obj.lossWeights = lossWeights;
            obj.physicsFcn  = physicsFcn;

            % 默认不做归一化
            obj.normIn  = struct('mode','none');
            obj.normOut = struct('mode','none');
        end

        %% 设置归一化参数（在 demo 中从训练数据统计后调用）
        function setNormalization(obj, normIn, normOut)
            if nargin >= 2 && ~isempty(normIn)
                obj.normIn = normIn;
            end
            if nargin >= 3 && ~isempty(normOut)
                obj.normOut = normOut;
            end
        end

        %% 物理域预测接口（方便 demo / 将来 S-function 使用）
        % u_phys: N×1 或 1×N (物理量)
        % v_phys: 同维度 (物理量)
        function v_phys = predictPhysical(obj, u_phys)
            u_row = u_phys(:)';   % 1 x N
            % 归一化输入
            u_norm = obj.applyNormIn(u_row);
            % 网络前向
            u_norm_dl = dlarray(single(u_norm));
            v_norm_dl = obj.forward(u_norm_dl);
            v_norm    = double(extractdata(v_norm_dl));
            % 反归一化输出
            v_row = obj.applyDenormOut(v_norm);
            v_phys = v_row(:);
        end

        %% ---------- 核心损失 ----------
        function [loss, grads] = computeLoss(obj, params, ...
                                             X_data, Y_data, X_phys, ...
                                             physicsFcn, physParam, lossWeights)
            % X_data, Y_data, X_phys 均在归一化空间
            % X_*: 1 x N, dlarray(single)
            % Y_*: 1 x N, dlarray(single)

            [L_data, L_phys, L_mono, L_smooth] = obj.computeLossTerms( ...
                params, X_data, Y_data, X_phys, physicsFcn, physParam, lossWeights);

            % -------------------------------
            % 总损失
            % -------------------------------
            loss = ...
                lossWeights.lambdaData   * L_data   + ...
                lossWeights.lambdaPhys   * L_phys   + ...
                lossWeights.lambdaMono   * L_mono   + ...
                lossWeights.lambdaSmooth * L_smooth;

            % -------------------------------
            % 梯度
            % -------------------------------
            grads = dlgradient(loss, params);
        end
        
        function Lvec = evalLossComponents(obj, params, ...
                                           X_data, Y_data, X_phys, ...
                                           physicsFcn, physParam, lossWeights)
            [L_data, L_phys, L_mono, L_smooth] = obj.computeLossTerms( ...
                params, X_data, Y_data, X_phys, physicsFcn, physParam, lossWeights);

            % ---------- 总损失 ----------
            L_total = ...
                lossWeights.lambdaData   * L_data + ...
                lossWeights.lambdaPhys   * L_phys + ...
                lossWeights.lambdaMono   * L_mono + ...
                lossWeights.lambdaSmooth * L_smooth;

            % 返回 vector（BasePINN 要求第一项必须是 total）
            Lvec = [L_total; L_data; L_phys; L_mono; L_smooth];
        end

    end

    methods (Access = private)
        function [L_data, L_phys, L_mono, L_smooth] = computeLossTerms(obj, params, ...
                X_data, Y_data, X_phys, physicsFcn, physParam, lossWeights)
            % -------------------------------
            % 1) 数据前向：v_pred_data_norm = f(u_norm_data)
            % -------------------------------
            v_pred_data_norm = BasePINN.forwardWithParams(params, X_data);

            % -------------------------------
            % 2) 数据项 L_data (归一化空间)
            % -------------------------------
            if lossWeights.lambdaData ~= 0
                L_data = mean((v_pred_data_norm - Y_data).^2, "all");
            else
                L_data = dlarray(single(0));
            end

            % -------------------------------
            % 3) 物理一致性项 L_phys: H(v_phys_pred) ≈ u_phys
            %    先把 u_norm, v_norm 转回物理空间
            % -------------------------------
            if lossWeights.lambdaPhys ~= 0
                % 3.1 归一化空间中预测
                v_pred_phys_norm = BasePINN.forwardWithParams(params, X_phys);

                % 3.2 反归一化到物理空间
                u_phys = obj.applyDenormIn(X_phys);           % u_phys
                v_phys = obj.applyDenormOut(v_pred_phys_norm);% v_phys

                % 3.3 调用正向迟滞模型得到 u_hat
                u_hat = physicsFcn(v_phys, physParam);        % 物理量

                % 3.4 在物理空间上计算残差
                L_phys = mean((u_hat - u_phys).^2, "all");
            else
                L_phys = dlarray(single(0));
            end

            % -------------------------------
            % 4) 单调性约束 L_mono （归一化空间 dv/du >= 0）
            % -------------------------------
            if lossWeights.lambdaMono ~= 0
                dv_du = dlgradient(sum(v_pred_data_norm, "all"), X_data);
                mono_penalty = relu(-dv_du);   % 只惩罚 dv/du<0 部分
                L_mono = mean(mono_penalty.^2, "all");
            else
                L_mono = dlarray(single(0));
            end

            % -------------------------------
            % 5) 输出平滑项 L_smooth （归一化空间）
            % -------------------------------
            if lossWeights.lambdaSmooth ~= 0
                v_seq = v_pred_data_norm;
                if size(v_seq,2) > 1
                    dv = v_seq(:,2:end) - v_seq(:,1:end-1);
                    L_smooth = mean(dv.^2, "all");
                else
                    L_smooth = dlarray(single(0));
                end
            else
                L_smooth = dlarray(single(0));
            end
        end
    
    end

    %% ========== 工具：dlarray 友好的归一化/反归一化 ==========
    methods (Access = public)
        % 对输入 u 做归一化 (网络输入前用)
        function u_n = applyNormIn(obj, u)
            % u: 1 x N (double or dlarray)
            mode = 'none';
            if isfield(obj.normIn, 'mode')
                mode = lower(obj.normIn.mode);
            end

            switch mode
                case 'zscore'
                    mu    = obj.normIn.mu;
                    sigma = obj.normIn.sigma;
                    if sigma == 0, sigma = 1; end
                    u_n = (u - mu) ./ sigma;

                case 'minmax'
                    xmin = obj.normIn.xmin;
                    xmax = obj.normIn.xmax;
                    denom = xmax - xmin;
                    if denom == 0, denom = 1; end
                    u_n = (u - xmin) ./ denom;

                otherwise
                    u_n = u;
            end
        end

        % 对输入的归一化值做反归一化 (物理项中恢复 u_phys)
        function u = applyDenormIn(obj, u_n)
            % u_n: 1 x N (dlarray)
            mode = 'none';
            if isfield(obj.normIn, 'mode')
                mode = lower(obj.normIn.mode);
            end

            switch mode
                case 'zscore'
                    mu    = obj.normIn.mu;
                    sigma = obj.normIn.sigma;
                    if sigma == 0, sigma = 1; end
                    u = u_n .* sigma + mu;

                case 'minmax'
                    xmin = obj.normIn.xmin;
                    xmax = obj.normIn.xmax;
                    denom = xmax - xmin;
                    if denom == 0, denom = 1; end
                    u = u_n .* denom + xmin;

                otherwise
                    u = u_n;
            end
        end

        % 对输出 v 做归一化 / 反归一化
        function v_n = applyNormOut(obj, v)
            mode = 'none';
            if isfield(obj.normOut, 'mode')
                mode = lower(obj.normOut.mode);
            end

            switch mode
                case 'zscore'
                    mu    = obj.normOut.mu;
                    sigma = obj.normOut.sigma;
                    if sigma == 0, sigma = 1; end
                    v_n = (v - mu) ./ sigma;

                case 'minmax'
                    xmin = obj.normOut.xmin;
                    xmax = obj.normOut.xmax;
                    denom = xmax - xmin;
                    if denom == 0, denom = 1; end
                    v_n = (v - xmin) ./ denom;

                otherwise
                    v_n = v;
            end
        end

        function v = applyDenormOut(obj, v_n)
            mode = 'none';
            if isfield(obj.normOut, 'mode')
                mode = lower(obj.normOut.mode);
            end

            switch mode
                case 'zscore'
                    mu    = obj.normOut.mu;
                    sigma = obj.normOut.sigma;
                    if sigma == 0, sigma = 1; end
                    v = v_n .* sigma + mu;

                case 'minmax'
                    xmin = obj.normOut.xmin;
                    xmax = obj.normOut.xmax;
                    denom = xmax - xmin;
                    if denom == 0, denom = 1; end
                    v = v_n .* denom + xmin;

                otherwise
                    v = v_n;
            end
        end
    end
end
