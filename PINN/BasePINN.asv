classdef BasePINN < handle
    %BASEPINN Generic PINN base class (增强版).
    %   - 网络参数 & 物理参数分开管理：
    %       paramsNet  : W1,b1,...,WL,bL
    %       paramsPhys : 物理参数（例如 GPI 的 eta, r, P, rd）
    %   - 前向传播 (MLP)
    %   - Adam 训练 + 热启动
    %   - dlfeval 通过静态 wrapper 统一调用
    %   - 支持 evalLossComponents 记录多项损失
    %   - 集成 mat_progress_bar 进度条
    %   - 自带 normalizeData / denormalizeData 工具
    %
    % 子类必须实现:
    %   [loss, gradsNet, gradsPhys] = computeLoss( ...
    %       obj, paramsNet, paramsPhys, ...
    %       X_data, Y_data, X_phys, ...
    %       physicsFcn, lossWeights)
    %
    % 可选实现（用于记录多项损失）:
    %   Lvec = evalLossComponents( ...
    %       obj, paramsNet, paramsPhys, ...
    %       X_data, Y_data, X_phys, ...
    %       physicsFcn, lossWeights)
    %   其中 Lvec = [L_total; L1; L2; ...].

    properties
        % 网络参数
        paramsNet       % 结构 W1,b1,...,WL,bL
        layers

        % 物理参数（由子类在构造函数中填充）
        paramsPhys      % 结构 (例如 eta, r, P, rd)

        % Adam 状态（网络参数）
        adamAvgNet
        adamAvgSqNet

        % Adam 状态（物理参数）
        adamAvgPhys
        adamAvgSqPhys

        % 训练迭代计数（支持热启动）
        iteration

        % 通用损失记录: struct('iter',[],'total',[],'components',[])
        lossHistory
    end

    methods
        %% ================== 构造函数 ==================
        function obj = BasePINN(layers)
            obj.layers = layers;

            % 1. 初始化网络参数
            obj.paramsNet = obj.initParams(layers);
            [obj.adamAvgNet, obj.adamAvgSqNet] = obj.initAdamState(obj.paramsNet);

            % 2. 物理参数起初为空（子类自行赋值）
            obj.paramsPhys   = struct();
            obj.adamAvgPhys  = struct();
            obj.adamAvgSqPhys= struct();

            % 3. 通用的 loss 记录结构
            obj.lossHistory = struct( ...
                'iter',      [], ...   % 1 x K
                'total',     [], ...   % 1 x K
                'components', [] );    % n_terms x K
            obj.iteration = 0;
        end

        %% ================== 前向预测 ==================
        % 使用当前 obj.paramsNet 做前向
        function Y = forward(obj, X)
            Y = BasePINN.forwardWithParams(obj.paramsNet, X);
        end

        %% ================== 训练（Adam，双参数集） ==================
        % X_data, Y_data, X_phys: 均为 dlarray(single)，1xN 或 d x N
        %
        % physicsFcn  : 正向物理算子句柄 (e.g. @(v,theta) GPI_forward_dl(...))
        % lossWeights : 结构体 {lambdaData, lambdaPhys, ...}
        % epochs      : 训练轮数
        % lrNet       : 网络参数学习率
        % lrPhys      : 物理参数学习率
        function obj = train(obj, X_data, Y_data, X_phys, ...
                             physicsFcn, lossWeights, ...
                             epochs, lrNet, lrPhys)

            if nargin < 9
                error('BasePINN.train: 缺少 lrNet 和 lrPhys。');
            end

            prev_len = 0;
            for ep = 1:epochs
                tic;
                obj.iteration = obj.iteration + 1;

                % ---------------- dlfeval 调用 computeLoss ----------------
                [loss, gradsNet, gradsPhys] = dlfeval(@BasePINN.lossWrapper, ...
                    obj, obj.paramsNet, obj.paramsPhys, ...
                    X_data, Y_data, X_phys, ...
                    physicsFcn, lossWeights);

                % ---------------- Adam 更新：网络参数 ----------------
                fnNet = fieldnames(obj.paramsNet);
                for i = 1:numel(fnNet)
                    name = fnNet{i};
                    [obj.paramsNet.(name), ...
                     obj.adamAvgNet.(name), ...
                     obj.adamAvgSqNet.(name)] = ...
                        adamupdate( ...
                            obj.paramsNet.(name), gradsNet.(name), ...
                            obj.adamAvgNet.(name), obj.adamAvgSqNet.(name), ...
                            obj.iteration, lrNet);
                end

                % ---------------- Adam 更新：物理参数 ----------------
                fnPhys = fieldnames(obj.paramsPhys);
                for i = 1:numel(fnPhys)
                    name = fnPhys{i};
                    [obj.paramsPhys.(name), ...
                     obj.adamAvgPhys.(name), ...
                     obj.adamAvgSqPhys.(name)] = ...
                        adamupdate( ...
                            obj.paramsPhys.(name), gradsPhys.(name), ...
                            obj.adamAvgPhys.(name), obj.adamAvgSqPhys.(name), ...
                            obj.iteration, lrPhys);
                end

                % ---------------- 记录损失（含多项组件） ----------------
                if ismethod(obj, 'evalLossComponents')
                    % Lvec: [L_total; L1; L2; ...]
                    Lvec = dlfeval(@BasePINN.evalWrapper, ...
                        obj, obj.paramsNet, obj.paramsPhys, ...
                        X_data, Y_data, X_phys, ...
                        physicsFcn, lossWeights);
                    Lvec = double(Lvec(:)).';  % 1 x n_terms

                    obj.lossHistory.iter(end+1)  = obj.iteration;
                    obj.lossHistory.total(end+1) = Lvec(1);

                    if isempty(obj.lossHistory.components)
                        obj.lossHistory.components = Lvec(:);
                    else
                        if size(obj.lossHistory.components,1) ~= numel(Lvec)
                            error('Loss component size mismatch between iterations.');
                        end
                        obj.lossHistory.components(:, end+1) = Lvec(:);
                    end
                else
                    % 只记录总损失
                    Ltotal = double(extractdata(loss));
                    obj.lossHistory.iter(end+1)  = obj.iteration;
                    obj.lossHistory.total(end+1) = Ltotal;
                end

                % ---------------- 进度条输出 ----------------
                t = toc;
                Lshow = double(extractdata(loss));
                prev_len = mat_progress_bar('Training', ep, epochs, t, prev_len, ...
                                            'Loss', Lshow);
            end
        end

        %% ================== 保存模型 ==================
        function saveModel(obj, filename)
            save(filename, 'obj');
        end

        %% ================== 抽象损失接口 ==================
        % 子类必须实现：返回标量 loss 以及针对 paramsNet, paramsPhys 的梯度结构
        function [loss, gradsNet, gradsPhys] = computeLoss(obj, paramsNet, paramsPhys, ...
                                                           X_data, Y_data, X_phys, ...
                                                           physicsFcn, lossWeights)
            
            error(['computeLoss must be implemented in subclass as ', ...
                   '[loss, gradsNet, gradsPhys] = computeLoss(obj, paramsNet, paramsPhys, ', ...
                   'X_data, Y_data, X_phys, physicsFcn, lossWeights).']);
        end
    end

    %% ================== 受保护的方法 ==================
    methods (Access = protected)
        % ---------- 参数初始化（等价于原 initPINNParams） ----------
        function params = initParams(~, layers)
            num_layers = numel(layers) - 1;
            params = struct();

            for i = 1:num_layers
                in_dim  = layers(i);
                out_dim = layers(i+1);

                limit = sqrt(6/(in_dim + out_dim));   % Xavier uniform

                W = rand(out_dim, in_dim)*2*limit - limit;
                b = zeros(out_dim, 1);

                params.(sprintf('W%d', i)) = dlarray(single(W));
                params.(sprintf('b%d', i)) = dlarray(single(b));
            end
        end

        % ---------- Adam 状态初始化 ----------
        function [stateAvg, stateAvgSq] = initAdamState(~, params)
            stateAvg   = struct();
            stateAvgSq = struct();

            fn = fieldnames(params);
            for i = 1:numel(fn)
                name = fn{i};
                p = params.(name);
                zeros_like = dlarray(zeros(size(p), 'like', p));
                stateAvg.(name)   = zeros_like;
                stateAvgSq.(name) = zeros_like;
            end
        end
    end

    %% ================== 静态方法（无状态工具函数） ==================
    methods (Static)
        % ---------- dlfeval 的统一入口（用于 computeLoss） ----------
        function [loss, gradsNet, gradsPhys] = lossWrapper(obj, paramsNet, paramsPhys, ...
                                                           X_data, Y_data, X_phys, ...
                                                           physicsFcn, lossWeights)
            [loss, gradsNet, gradsPhys] = obj.computeLoss( ...
                paramsNet, paramsPhys, ...
                X_data, Y_data, X_phys, ...
                physicsFcn, lossWeights);
        end

        % ---------- dlfeval 入口（用于 evalLossComponents） ----------
        function Lvec = evalWrapper(obj, paramsNet, paramsPhys, ...
                                    X_data, Y_data, X_phys, ...
                                    physicsFcn, lossWeights)
            Lvec = obj.evalLossComponents( ...
                paramsNet, paramsPhys, ...
                X_data, Y_data, X_phys, ...
                physicsFcn, lossWeights);
        end

        % ---------- 前向传播（等价于原 forwardPINN） ----------
        function Y = forwardWithParams(paramsNet, X)
            % 确保 X 是 dlarray(single)
            if ~isa(X, 'dlarray')
                X = dlarray(single(X));
            else
                if ~isa(extractdata(X), 'single')
                    X = dlarray(single(extractdata(X)));
                end
            end

            % 自动统计层数
            num_layers = 0;
            while isfield(paramsNet, sprintf('W%d', num_layers + 1))
                num_layers = num_layers + 1;
            end

            A = X;
            % 前 num_layers-1 层 tanh
            for i = 1:(num_layers-1)
                W = paramsNet.(sprintf('W%d', i));
                b = paramsNet.(sprintf('b%d', i));

                Z = W*A + b;
                A = tanh(Z);
            end

            % 最后一层线性
            W = paramsNet.(sprintf('W%d', num_layers));
            b = paramsNet.(sprintf('b%d', num_layers));

            Y = W*A + b;
        end

        % ========= 通用工具函数：归一化 / 反归一化 =========
        function [Xn, normInfo] = normalizeData(X, mode, normInfo)
            %NORMALIZEDATA  对列样本数据做归一化（特征在行）
            %
            %   [Xn, normInfo] = BasePINN.normalizeData(X, mode)
            %   [Xn, normInfo] = BasePINN.normalizeData(X, mode, normInfo)
            %
            %   mode: 'none' | 'zscore' | 'minmax'
            %   X   : (d x N) 矩阵，d 维特征，N 个样本
            %
            %   normInfo 结构:
            %     .mode  = mode
            %   对于 'zscore':
            %     .mu    : d x 1
            %     .sigma : d x 1
            %   对于 'minmax':
            %     .xmin  : d x 1
            %     .xmax  : d x 1

            if nargin < 2 || isempty(mode)
                mode = 'none';
            end
            if nargin < 3
                normInfo = struct();
            end

            Xn = X;
            modeLower = lower(mode);

            switch modeLower
                case 'none'
                    normInfo.mode = 'none';

                case 'zscore'
                    if ~isfield(normInfo, 'mu') || ~isfield(normInfo, 'sigma')
                        normInfo.mu    = mean(X, 2);
                        normInfo.sigma = std(X, 0, 2);
                        % 防止除 0
                        normInfo.sigma(normInfo.sigma == 0) = 1;
                    end
                    normInfo.mode = 'zscore';
                    Xn = (X - normInfo.mu) ./ normInfo.sigma;

                case 'minmax'
                    if ~isfield(normInfo, 'xmin') || ~isfield(normInfo, 'xmax')
                        normInfo.xmin = min(X, [], 2);
                        normInfo.xmax = max(X, [], 2);
                    end
                    normInfo.mode = 'minmax';

                    denom = normInfo.xmax - normInfo.xmin;
                    denom(denom == 0) = 1;

                    Xn = (X - normInfo.xmin) ./ denom;

                otherwise
                    error('BasePINN:normalizeData', ...
                          'Unknown normalization mode "%s".', mode);
            end
        end

        function X = denormalizeData(Xn, normInfo)
            %DENORMALIZEDATA  根据 normInfo 做反归一化
            %
            %   X = BasePINN.denormalizeData(Xn, normInfo)

            if ~isfield(normInfo, 'mode') || strcmpi(normInfo.mode, 'none')
                X = Xn;
                return;
            end

            modeLower = lower(normInfo.mode);

            switch modeLower
                case 'zscore'
                    mu    = normInfo.mu;
                    sigma = normInfo.sigma;
                    sigma(sigma == 0) = 1;
                    X = Xn .* sigma + mu;

                case 'minmax'
                    xmin = normInfo.xmin;
                    xmax = normInfo.xmax;
                    denom = xmax - xmin;
                    denom(denom == 0) = 1;
                    X = Xn .* denom + xmin;

                otherwise
                    error('BasePINN:denormalizeData', ...
                          'Unknown normalization mode "%s".', mode);
            end
        end
    end
end
